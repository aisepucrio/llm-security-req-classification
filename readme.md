# Evaluating the Potential of Large Language Models in Security-Related Software Requirements Classification

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.16001526.svg)](https://doi.org/10.5281/zenodo.16001526)

This project contains the replication package for the paper "Evaluating the Potential of Large Language Models in Security-Related Software Requirements Classification".

This paper explores the effectiveness of Large Language Models (LLMs) in classifying security-related software requirements. Utilizing the SecReq, DOSSPRE, and PROMISE+ datasets.

The pre-print for the paper is available [here](paper/LLMSecReq_2025.pdf).

## Repository Structure

- `/paper/` - Contains the pdf to the paper.
- `/DataSets/` - Contains the datasets that were used as input for the study. Our [license](LICENSE) does not apply to these datasets.
- `/source/` - Contains the scripts used to execute and replicate the study. 
- `/ConsolidatedData/`, `/results/` - Contains the intermediary and resulting data files that were generated as we executed our scripts. 

## Requirements

- Python 3.12 (may work on other versions, but your mileage may vary)
- Ollama (for running local models, see Model Configuration for more details on installation)
   - Note that ollama requires a modern CPU (4+ cores, preferably with AVX512 support), a sizeable amount of RAM (at least 32GB to confortably run the models we have used), at least 20-100 GBs of free disk space, and preferably (but optional) a modern GPU with at least 12GBs of VRAM (for reasonable performance, but 8GB GPUs should work aswell).
- An OpenAI API Key (paid, for running GPT models)

### Model configuration
The following models are used in this project. The models and their specification are accessible through the provided links:
- **Llama 3:** https://ollama.com/library/llama3:8b
- **Llama 3.1:** https://ollama.com/library/llama3.1
- **Llama 3.2 - Vision:** https://ollama.com/library/llama3.2-vision
- **Mistral:** https://ollama.com/library/mistral
- **Mistral-Nemo:** https://ollama.com/library/mistral-nemo
- **Mistral-Small:** https://ollama.com/library/mistral-small
- **Gemma:** https://ollama.com/library/gemma
- **Gemma 2:** https://ollama.com/library/gemma2:27b
- **DeepSeek-R1-Distill-Qwen-14B:** https://ollama.com/library/deepseek-r1:14b
- **GPT-4o Mini:** https://platform.openai.com/docs/models

The execution script assumes that ollama is running on localhost and has all of the local models installed.

Each of the models can be installed by utilizing the command `ollama pull <model_name>`. `model_name` can be the the following: 
   - llama3:8b
   - llama3.1
   - llama3.2-vision
   - mistral
   - mistral-nemo
   - mistral-small
   - gemma
   - gemma2:27b
   - deepseek-r1:14b

In terms of hardware requirements each model requires combined system memory (RAM + GPU VRAM) that is approximately 60% of the number of parameters (in billions). For example, deepseek-r1:14b has 14,8B parameters, and requires roughly 9 gigabytes of available memory.

## Installation and Execution

These are the steps for installation and execution of the scripts. The result files generated by steps 2, 3, 4, and 5 are contained in this repository. 
   - Some of the log files, most specifically those contained in [results/logs_extra_datasets](results/logs_extra_datasets) are compressed due to their large file size.git
   - All of the commands listed below expect that your current working directory is the root of the repository.
   - In some operating systems, you may need to use another alias for the `python` command, such as `python3` or `py`.

1. **Set Up the Environment**  
   - Create a virtual environment and install the packages listed in `requirements.txt`.
   - `pip install -r requirements.txt`

2. **Install Ollama**  
   - Install Ollama and start the Ollama server.

3. **Create a Consolidated Dataset**  
   - Run `python source/create_consolidated_dataset.py` to generate the consolidated dataset.

4. **Generate Model Predictions**  
   - Run `python source/main.py` to generate predictions from the models.
     - The `main` script contains a list of models. To evaluate new models:
       - Add the model name to the list. 
       - Ensure the model meets one of the following criteria:
         - It is from OpenAI (its name must include "gpt" as it stands). GPT models require that you fill the `OPENAI_API_KEYS` variable at the beginning of the script. It supports multiple keys, and will rotate between them as needed.
         - It is available in the Ollama library. Missing models defined for evaluation should be automatically pulled by the Ollama provider.

5. **Consolidate the Results**
   - Run `python source/consolidate_results.py` to generate a results file which contains only data from the three datasets that were in the final analysis: secreq, dosspre, and promise+.

6. **Generate Metrics for Analysis**  
   - Run `python source/results_metrics.py` to create the evaluation metrics used for analysis.
     - This script also contains a model list. Update it to specify which models you want to include in the metrics generation.
     - Some of the functions generate latex tables that were utilized in the paper. These functions are commented at the end of the file.
     - The generated DataFrames are passed to functions in the `source/data_analysis.py` file. Each function corresponds to one of the tables displayed in the results section.

## Prompt Design
Prompts are generated in `source/prompt.py`. The factory injects each requirement into the specified strategy template and returns it as a user message.  

To add new prompts:
1. Add the template to the prompt factory in `source/prompt.py`.
2. Update the `strategies` list at the end of the file.

## Paper to Script Mapping

This section details how the `source/results_metrics.py` script and its functions relate to the tables in the paper.

- `get_rq1_consolidated_result(labels, models)`: This function is used for RQ1. It processes the results from the models using the zero-shot prompting strategy and calculates the precision, recall, and F1-score metrics. The data this function generates is the basis for the "Zero-shot" section of **Table 3**.

- `get_rq2_consolidated_result(labels, models)`: This function is used in RQ2. It performs the same metric calculations as get_rq1_consolidated_result but for the other prompting strategies: Few-shot, Auto-CoT, and Raw-inst. The data from this function populates the corresponding sections of **Table 3**.

- The main function also contains commented-out calls to several functions from the `source/data_analysis.py` script, such as `rq1_eda(df_rq1)` and `rq2_improvement_by_strategy(df_rq1,df_rq2)`. They generate the data for the remaining tables, specifically a more detailed version of **Table 3** and **Table 4**, respectively.

- `rq2_statistical_test(df_rq2)`: This function performs the statistical analysis for RQ2. Although this is a function from data_analysis.py, its call here in results_metrics.py is what triggers the creation of the data for **Table 5**.

- `get_rq3_consolidated_secreq_result(labels, models)`: This functions generates the consolidated metrics for the SecReq dataset. This data would be used to create **Table 6**, which shows the top-5 performing models and strategies specifically for this dataset.

- `get_rq3_individual_secreq_result(labels, models)`: This function is intended to be part of the analysis for RQ3. Uncomment it to calculate the metrics for each individual SecReq project (CPN, ePurse, and GP), providing the data for the detailed comparative analysis in **Table 7**.


## License
This project is licensed under the MIT License.
Feel free to use, modify, and distribute it as permitted under the terms of this license.

This license is not applicable to the files available in the datasets folder, as they are replicated from other works (see the paper for their sources). 
These datasets were included in this repository for preservation purposes, and to allow this replication package to be self-contained in terms of inputs.